{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithms (DGA) \n",
    "\n",
    "MITRE ATT&CK: https://attack.mitre.org/techniques/T1323/\n",
    "\n",
    "## Preamble\n",
    "To combat the use of blacklists on hard-coded domain-names, a domain generation algorithm is often coded into Malware samples.  This technique allows adversaries generate domain names and effectively bypass any blocking behaviours.\n",
    "\n",
    "Traditional approaches are reactive - it involves reverse engineering the Malware binaries.  This is naturally time consuming.\n",
    "\n",
    "## Detection Approach(s)\n",
    "\n",
    "* NXDomains\n",
    "If a single host is making a large number of NXDomain (non-existant) - this could indicate Malware usage.  This is still a reactive approach, it requires the sum of NX domains to be met before approach could be made.\n",
    "\n",
    "* Feature Engineering\n",
    "Look at intrinsic features\n",
    "\n",
    "* RNN\n",
    "Scalable and easy to use technique for detection\n",
    "\n",
    "## Datasets\n",
    "* Cisco's Umbrella Popularity\n",
    " The popularity list contains our most queried domains based on passive DNS usage across our Umbrella global network of more than 100 Billion requests per day with 65 million unique active users, in more than 165 countries. Unlike Alexa, the metric is not based on only browser based 'http' requests from users but rather takes in to account the number of unique client IPs invoking this domain relative to the sum of all requests to all domains. In other words, our popularity ranking reflects the domain’s relative internet activity agnostic to the invocation protocols and applications where as ’site ranking’ models (such as Alexa) focus on the web activity over port 80 mainly from browsers. \n",
    "https://s3-us-west-1.amazonaws.com/umbrella-static/index.html\n",
    "\n",
    "\n",
    "* DGA Sets from NetLab\n",
    "\n",
    "\n",
    "## References\n",
    "[1] - https://blog.malwarebytes.com/security-world/2016/12/explained-domain-generating-algorithm/\n",
    "\n",
    "[2] - https://www.youtube.com/watch?v=jm7wH2G0h6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import sklearn\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>domain</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cisco</td>\n",
       "      <td>api-global.netflix.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cisco</td>\n",
       "      <td>clients4.google.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cisco</td>\n",
       "      <td>mtalk.google.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>nymaim</td>\n",
       "      <td>cdfviq.com</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nymaim</td>\n",
       "      <td>meisvfddhoc.biz</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       src                  domain      class\n",
       "2    cisco  api-global.netflix.com     benign\n",
       "14   cisco     clients4.google.com     benign\n",
       "41   cisco        mtalk.google.com     benign\n",
       "43  nymaim              cdfviq.com  malicious\n",
       "26  nymaim         meisvfddhoc.biz  malicious"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting Benign Domains\n",
    "benignDomains_df = pd.read_csv('dataset/CISCO-top-1m.csv', names=['domain'])\n",
    "benignDomains_df['class'] = 'benign'\n",
    "benignDomains_df['src'] = 'cisco'\n",
    "\n",
    "# Ingesting Malicious Domains\n",
    "maliciousDomains_df = pd.read_csv('dataset/dga-domains.txt', sep='\\t', usecols=[0,1], names=['src', 'domain'])\n",
    "maliciousDomains_df['class'] = 'malicious'\n",
    "\n",
    "# Merging Datasets\n",
    "dataset_df = pd.concat([maliciousDomains_df[0:50], benignDomains_df[0:50]], sort=False)\n",
    "dataset_df = dataset_df.sample(frac=1)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_features):\n",
    "    \"\"\"Builds logistic regression model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=max_features, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing data\n",
      "Count Vec:   (0, 240)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 365)\t1\n",
      "  (0, 158)\t1\n",
      "  (0, 191)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 322)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 219)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 185)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 231)\t1\n",
      "  (0, 194)\t1\n",
      "  (0, 120)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 139)\t1\n",
      "  (0, 258)\t1\n",
      "  (0, 32)\t1\n",
      "  (1, 88)\t1\n",
      "  (1, 190)\t1\n",
      "  (1, 236)\t1\n",
      "  (1, 242)\t1\n",
      "  :\t:\n",
      "  (98, 402)\t1\n",
      "  (98, 387)\t1\n",
      "  (98, 248)\t1\n",
      "  (98, 339)\t1\n",
      "  (98, 316)\t1\n",
      "  (98, 166)\t1\n",
      "  (98, 320)\t1\n",
      "  (98, 71)\t1\n",
      "  (98, 236)\t1\n",
      "  (98, 122)\t1\n",
      "  (98, 98)\t1\n",
      "  (98, 219)\t1\n",
      "  (98, 11)\t1\n",
      "  (99, 39)\t1\n",
      "  (99, 181)\t1\n",
      "  (99, 86)\t1\n",
      "  (99, 94)\t1\n",
      "  (99, 378)\t1\n",
      "  (99, 281)\t1\n",
      "  (99, 319)\t1\n",
      "  (99, 190)\t1\n",
      "  (99, 98)\t1\n",
      "  (99, 219)\t1\n",
      "  (99, 11)\t1\n",
      "  (99, 29)\t1\n",
      "Y: [0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1]\n",
      "Labels: ['benign', 'benign', 'benign', 'malicious', 'malicious', 'benign', 'malicious', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'malicious', 'benign', 'malicious', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'benign', 'benign', 'benign', 'malicious', 'malicious', 'malicious', 'benign', 'malicious', 'malicious', 'malicious', 'malicious', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'malicious', 'benign', 'benign', 'malicious', 'benign', 'malicious', 'benign', 'benign', 'benign', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'benign', 'benign', 'benign', 'benign', 'benign', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'benign', 'benign', 'benign', 'malicious', 'benign', 'benign', 'benign', 'benign', 'benign', 'malicious', 'malicious', 'malicious', 'benign', 'benign', 'benign', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'malicious', 'benign', 'benign', 'malicious', 'benign', 'malicious', 'malicious', 'malicious']\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 0.6801\n",
      "Ready to predict\n"
     ]
    }
   ],
   "source": [
    "def run(max_epoch=5, nfolds=10, batch_size=20):\n",
    "    \"\"\"Run train/test on logistic regression model\"\"\"\n",
    "\n",
    "    X = list(dataset_df['domain'])\n",
    "    labels = list(dataset_df['class'])\n",
    "    \n",
    "    # Create feature vectors\n",
    "    print(\"vectorizing data\")\n",
    "    ngram_vectorizer = feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "    count_vec = ngram_vectorizer.fit_transform(X)\n",
    "\n",
    "    max_features = count_vec.shape[1]\n",
    "    \n",
    "    # Create standard labels:\n",
    "    y = [0 if x == 'benign' else 1 for x in labels]\n",
    "    \n",
    "    for fold in range(nfolds):\n",
    "        \n",
    "\n",
    "        print('Count Vec: ' + str(count_vec))\n",
    "        print('Y: ' + str(y))\n",
    "        print('Labels: ' + str(labels))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test, _, label_test = train_test_split(count_vec, y, labels, test_size=0.2)\n",
    "        \n",
    "        #print('Building Model...')\n",
    "        model = build_model(max_features)\n",
    "        \n",
    "        #print('Training Model...')\n",
    "        X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)\n",
    "        \n",
    "        best_iter = -1\n",
    "        best_auc = 0.0\n",
    "        out_data = {}\n",
    "        \n",
    "        for ep in range(max_epoch):\n",
    "            model.fit(X_train.todense(), y_train, batch_size=batch_size, epochs=1)\n",
    "            \n",
    "            t_probs = model.predict_proba(X_holdout.todense())\n",
    "            t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)\n",
    "\n",
    "            if t_auc > best_auc:\n",
    "                \n",
    "                print('Ready to predict')\n",
    "                return model\n",
    "\n",
    "            else:\n",
    "                # No longer improving...break and calc statistics\n",
    "                if (ep-best_iter) > 5:\n",
    "                    break\n",
    "\n",
    "testModel = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-29aee1356717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mngram_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tedsfwfkewrglkergst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(testModel.predict(count_vec))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[1;32m    297\u001b[0m                         \" or shape[0]\")\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "count_vec = ngram_vectorizer.fit_transform(['tedsfwfkewrglkergst'])\n",
    "print(len(count_vec))\n",
    "#print(testModel.predict(count_vec))\n",
    "\n",
    "# print(testModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
